---
pagetitle: Combing n-gram grammars
---

# Combining $n$-gram grammars

:::prereqs
- sets (basic notation, operations)
- strings (basic notation)
:::

So far, our short expedition has resulted in a few formal insights into $n$-gram grammars, but also a variety of examples of how they can be used in linguistics.
However, all the examples were limited to specific phenomena.
If $n$-gram grammars are supposed to provide a model of phonotactics or morphotactics, it isn't enough to show that individual phenomena in those domains can be described with those grammars.
We want to know how we can turn these collections of grammars for individual phenomena into a single grammar.
For example, we already know that word-final devoicing can be handled by $n$-gram grammars, and the same is true for penultimate stress.
But a speaker whose language displays both phenomena must have some grammar that takes care of both.
Is this combined grammar also an $n$-gram grammar?
It is not immediately obvious that the answer to that is yes.

There are other types of grammars for which the answer is no.
Later on, we will encounter context-free grammars (CFGs).
<!-- fixme: will we actually encounter them? -->
For CFGs, it is not the case that what can be jointly accomplished by two grammars can be done by a single grammar.
The problem is mostly limited to artificial languages (which is interesting in itself --- apparently all natural languages have some special property that avoids this problem).
But it nonetheless shows that one cannot model each phenomenon with its own grammar and simply assume that they can all be combined into a single grammar of the same type.

::: example
Consider the following three string languages:

- $L_\mathit{ab} \is a^n b^n c^*$
- $L_\mathit{bc} \is a^* b^n c^n$
- $L_\mathit{abc} \is a^n b^n c^n$

All three contain only strings that are built over *a*, *b*, and *c* such that all *a*s precede all *b*s and all *b*s precede all *c*s.
But they differ in the additional restrictions that are met by the strings:

- $L_\mathit{ab}$: the number of *a*s must match the number of *b*s
- $L_\mathit{bc}$: the number of *b*s must match the number of *c*s
- $L_\mathit{abc}$: both of the matching conditions above

Here is a table with a few example strings and whether they are contained in the language or not.

| **String**       | $L_\mathit{ab}$ | $L_\mathit{bc}$ | $L_\mathit{abc}$ |
| --:              | :-:             | :-:             | :-:              |
| $\emptystring$   | $\in$           | $\in$           | $\in$            |
| a                | $\notin$        | $\notin$        | $\notin$         |
| ab               | $\in$           | $\notin$        | $\notin$         |
| bbcc             | $\notin$        | $\in$           | $\notin$         |
| aaabbbccc        | $\in$           | $\in$           | $\in$            |

It is easy to show that there is a CFG that generates $L_\mathit{ab}$, and similarly that there is a CFG that generates $L_\mathit{bc}$.
Since we haven't discussed CFGs yet, we won't show those grammars here.
<!-- fixme: yet? we might never -->
The important thing is that even though $L_\mathit{ab}$ and $L_\mathit{bc}$ can be generated by CFGs, $L_\mathit{abc}$ cannot!
Even though each individual matching condition can be captured by a CFG, their combination cannot.
:::

Fortunately, $n$-gram grammars are not like CFGs and can be combined in a very simple fashion.

## Combining negative grammars

Suppose you have two negative bigram grammars that you want to combine into a single bigram grammar.
The first thing to be clarified here is what we mean by combine.
Given two grammars $G$ and $G'$, their combination could accept every string that is generated by both grammars, or every string that is generated by at least one grammar.
The former is much more restrictive than the latter.
It is like saying "each one of these grammars imposes a constraint, and we combine the constraints into a single grammar that enforces them all at once".
The other option instead allows for violations: "there's a bunch of constraints, and as long as a string obeys at least one of them, it is well-formed".
This is not how natural languages work.
If a language has both word-final devoicing and intervocalic voicing, then a word has to satisfy both.
You cannot say "Oh, I already did the intervocalic voicing, so let's not bother with the word-final devoicing".
Language is like an exam where you get an F unless you answer every single question correctly.

Because of this, we only have to consider the stricter way of combining grammars, the one where the resulting grammar only generates strings that are well-formed with respect to each one of the original grammars.
We also say that the language of this grammar is the **intersection** of the languages generated by the original grammars.
Such a combined grammar is actually very easy to build: construct a new negative $n$-gram grammar that contains every $n$-gram that belongs to at least one of the original negative grammars.
In set-theoretic terms, this amounts to taking the **union** of the two negative grammars.

::: example
Suppose that we have a negative bigram grammar $G_1$ for word-final devoicing that contains only the bigram *s{{{R}}}*.
In set notation, $G_1 \is \setof{\text{s}{{{R}}}}$.
This grammar rules out every string that ends in an *s*.
Furthermore, let $G_2$ be a negative trigram grammar for intervocalic voicing.
It contains the trigrams *asa*, *asi*, *isa*, and *isi* (a realistic grammar would of course require more than just those four trigrams as the language would have more than just those two vowels).
So $G_2$ forbids every string where *s* occurs between the vowels *a* and *i*.


In order to obtain a grammar that enforces both constraints, one simply constructs a new grammar $G$ that contains all these $n$-grams: *s{{{R}}}*, *asa*, *asi*, *isa*, *isi*.
Every string that is ruled out by $G_1$ or $G_2$ is also ruled out by $G$.
And every string that is allowed by $G$ is allowed by both $G_1$ and $G_2$.
If desired, we can convert $G$ from a mixed grammar to a fixed one using the familiar padding procedure from an earlier unit.
:::

We can express this insight in terms of a theorem.

::: theorem
Let $G_1$ be a negative $m$-gram grammar and $G_2$ a negative $n$-gram grammar (where $m \geq 1$ and $n \geq 1$).
Then $L(G_1) \cap L(G_2) = L(G_1 \cup G_2)$.
:::

Let us look in detail at how this theorem uses mathematical notation to express our idea about combining grammars.
Recall that $L(G)$ is the set of strings that are well-formed with respect to $G$.
The intersection $A \cap B$ of two sets $A$ and $B$ contains all elements that occur in both $A$ and $B$.
This is in contrast to the union $A \cup B$, which contains all elements that occur in at least one of the two, $A$ or $B$.
So $L(G_1) \cap L(G_2)$ is the set of strings that belong to both $L(G_1)$ and $L(G_2)$.
In other words, these strings are well-formed with respect to both $G_1$ and $G_2$.
The theorem tells us that these are exactly the strings that are well-formed with respect to some other grammar that is the union of $G_1$ and $G_2$.
Talking about the union of two grammars makes sense because we have defined $n$-gram grammars as finite sets --- set-theoretic operations like union can be applied to these grammars like to any other set.

That negative $n$-gram grammars are this easy to combine is a huge advantage.
It means that when faced with a complex system like natural language phonotactics, we can safely decompose it into simpler subsystems and design a grammar for each one of them.
For instance, a grammar for word-final devoicing, another for intervocalic voicing, another for another for enforcing syllable templates, another to ensure that primary stress falls on the last syllable, and so on.
The overall system is then obtained in a purely mechanical fashion by taking the union of the grammars for these subsystems.
Not only does this greatly simplify the linguistic analysis step, it also helps with real-world implementations.
For each phenomenon one designs a small grammar that can be easily tested for correctness, and only once all components have been individually verified does one combine them into the full model.

::: exercise
Write two negative grammars with alphabet $\setof{a,b}$ such that

- the first grammar only generates *ab*, *abb*, *abbb*, and so on, and
- the second grammar generates all strings of the form *ab*, *aab*, *aaab*, and so on.

Then build the corresponding combined grammar.
What is the language generated by the combined grammar?
Is this the correct result?

::: solution
1. $G_1 \is \setof{ {{{L}}}{{{R}}}, {{{L}}}b, aa, ba, a{{{R}}}}$
1. $G_2 \is \setof{ {{{L}}}{{{R}}}, {{{L}}}b, ba, bb, a{{{R}}}}$
1. $G_1 \cup G_2 \is \setof{ {{{L}}}{{{R}}}, {{{L}}}b, aa, ba, bb, a{{{R}}} }$
1. $L(G_1 \cup G_2)$ contains only the string *ab*, which is the correct result as $L(G_1) \cap L(G_2) = \setof{\mathit{ab}}$.

::: solution_explained
This exercise is slightly cumbersome because it asks you to provide negative grammars when positive grammars would be easier.
But of course you can always start out with positive grammars and then convert them to negative ones.

The first language contains *abbb*, so all of the following must be allowed: {{{L}}}a, ab, bb, b{{{R}}}.
In fact, these are the only bigrams that can ever occur in *ab*, *abb*, *abbb*, and so on.
Converting this positive grammar into a negative one gives us $G_1$ above.

Similarly, the second language contains *aaab* and thus all of the following must be allowed: {{{L}}}a, aa, ab, b{{{R}}}.
Again these are all the bigrams that can ever occur in *ab*, *aab*, *aaab*, and so on, and converting the grammar to a negative one yields $G_2$ above.

When we combine the two grammars, we take their union.
Since the two grammars only differ in whether they contain **aa** or *bb*, their union looks almost exactly the same.
The resulting grammar requires that strings do not start with *b* or end *a*, don't have *a* after *b*, and don't have multiple *a*s or *b*s in a row.
Also, it rules out the empty string.
This leaves *ab* as the only well-formed string, and this is indeed the only string in the intersection of the two languages.
:::
:::
:::

::: exercise
Write two negative grammars with alphabet $\setof{a,b}$ such that

- the first generates all strings of the form $(\String{ab})^+$, i.e. *ab*, *abab*, *ababab*, and so on,
- the second only generates $\String{a^+ b}$, i.e. *ab*, *aab*, *aaab*, and so on.

Then build the corresponding combined grammar.
What is the language generated by the combined grammar?
Is this the correct result?

::: solution
1. $G_1 \is \{ {{{L}}}{{{R}}}, {{{L}}}b, aa, bb, a{{{R}}} \}$
1. $G_2 \is \{ {{{L}}}{{{R}}}, {{{L}}}b, ba, bb, a{{{R}}} \}$
1. $G_1 \cup G_2 \is \{ {{{L}}}{{{R}}}, {{{L}}}b, aa, ba, bb, a{{{R}}} \}$

The language of $G_1 \cup G_2$ consists only of *ab*, which is indeed the only string generated by both of the original grammars.

::: solution_explained
By now you should not have had too much trouble designing $G_1$ and $G_2$, in particular since we have already encountered $G_2$ in the previous exercise.
The combined grammar is just their union.

While this is a simple example, it can still be helpful to make the solution process more visual via a table.
The table below represents the set of all possible bigrams over *a* and *b* with edge markers.
In this table, each row indicates the first symbol of the bigram and each column the second symbol (since {{{L}}} cannot be the second symbol of a useful bigram, it does not need a column, and since {{{R}}} cannot be the first symbol it does not need a row).
Cells are filled with 1 or 2 (or both) depending on which grammar the corresponding bigram occurs in.
The combined grammar is obtained by taking all bigrams whose cell isn't empty (i.e. which occur in at least one of the two grammars).

|             |  **a** | **b** | **{{{R}}}** |
| --:         | :-:    | :-:   | :-:         |
| **{{{L}}}** |        | 1, 2  | 1, 2        |
| **a**       |  1     |       | 1, 2        |
| **b**       |  2     | 1, 2  |             |

This table shows very clearly that even though the languages of $G_1$ and $G_2$ are very different, the grammars differ only in which of *aa* and *ba* they contain.
The combined grammar with both then is so limited that only *ab* can be generated.
:::
:::
:::

::: exercise
Write two negative grammars with alphabet $\setof{a,b,c}$ such that

- the first contains all strings except those that start with *c* or end in *c* (for instance *abb* or *acbccb* but not *cbccb*, *acbcc*, or *cbcc*),
- the latter generates all strings of the form $a^+ b^* c^*$, i.e. 1 or more *a*s followed by 0 or more *b*s, followed by 0 or more *c*s (for instance *a*, *aaaab*, *aaabc*, or *accccc*, but not *bbcc* or *aaacb*).

Then build the corresponding combined grammar.
What is the language generated by the combined grammar?
Is this the correct result?

::: solution
1. $G_1 \is \setof{ {{{L}}}c, c{{{R}}} }$
1. $G_2 \is \setof{ {{{L}}}{{{R}}}, {{{L}}}b, {{{L}}}c, ba, ca, cb}$
1. $G_1 \cup G_2 \is \setof { {{{L}}}{{{R}}}, {{{L}}}b, {{{L}}}c, ba, ca, cb, c{{{R}}} }$

The combined grammar generates the set of non-empty strings of the form $a^+ b^*$, i.e. 1 or more *a*s followed by 0 or more $b$s.
This is the correct result.

::: solution_explained
Again it can be convenient to use the tabular format above for representing the relevant bigrams.

|             |  **a** | **b** | **c** | **{{{R}}}** |
| --:         | :-:    | :-:   | :-:   | :-:         |
| **{{{L}}}** |        |       | 1     | 2           |
| **a**       |        |       |       |             |
| **b**       |  2     |       |       |             |
| **c**       |  2     | 2     |       | 1           |

Notably, the combined grammar makes it impossible for *c* to occur at all even though it does not contain all bigrams with *c*.
:::
:::
:::


## Extension to positive grammars via De Morgan's Law

Combining grammars by taking their union only gives the desired result for negative grammars.
For positive grammars, one has to take their intersection instead.
(And since positive grammars always require a fixed $n$-gram length, one has to make sure first that the $n$-grams have the same length across all grammars.)

::: theorem
Let $G_1$ and $G_2$ be positive $n$-gram grammars.
Then $L(G_1) \cap L(G_2) = L(G_1 \cap G_2)$.
:::

Think about this theorem for a moment to figure out the intuition behind it.

...

...

...

Done?
Okay, you probably found the intuitive answer that in order to get the set of strings that are allowed by both grammars, we only want to keep the $n$-grams that are allowed by both grammars.
That's great, but there is also a very different way to think about it.
The set operations of intersection and union are intimately connected via another operation called relative complement.
The complement of $B$ with respect to $A$, written $A - B$, contains all elements of $A$ that aren't also members of $B$.
Given some fixed universe $U$, one usually writes $\overline{A}$ instead of $U - A$.
It then holds that $A \cap B = \overline{\overline{A} \cup \overline{B}}$ and $A \cup B = \overline{\overline{A} \cap \overline{B}}$.
This is known as **De Morgan's law**.

::: example
Let $A \is \setof{a,b,c}$ and $B \is \setof{b,c,d}$, while the fixed universe is $U \is \setof{a,b,c,d,e}$.
Then $A \cup B = \setof{a,b,c} \cup \setof{b,c,d} = \setof{a,b,c,d}$.
Following De Morgan's law, we can also compute it in a stepwise fashion as $\overline{\overline{A} \cap \overline{B}}$:


- $\overline{A} = U - A = \setof{a,b,c,d,e} - \setof{a,b,c} = \setof{d,e}$
- $\overline{B} = U - B = \setof{a,b,c,d,e} - \setof{b,c,d} = \setof{a,e}$
- $\overline{A} \cap \overline{B} = \setof{e}$
- $\overline{\overline{A} \cap \overline{B}} = U - (\overline{A} \cap \overline{B}) = \setof{a,b,c,d,e} - \setof{e} = \setof{a,b,c,d}$


As you can see, $A \cup B = \setof{a,b,c,d} = \overline{\overline{A} \cap \overline{B}}$.
:::

::: exercise
Compute $A \cap B$ in the same fashion to show that it is equivalent to $\overline{\overline{A} \cup \overline{B}}$.

::: solution
According to De Morgan's Law, $A \cap B = \overline{\overline{A} \cup \overline{B}}$

- $\overline{A} = U - A = \setof{a,b,c,d,e} - \setof{a,b,c} = \setof{d,e}$
- $\overline{B} = U - B = \setof{a,b,c,d,e} - \setof{b,c,d} = \setof{a,e}$
- $\overline{A} \cup \overline{B} = \setof{a,d,e}$
- $\overline{\overline{A} \cup \overline{B}} = U - (\overline{A} \cup \overline{B}) = \setof{a,b,c,d,e} - \setof{a,d,e} = \setof{b,c}$
:::
:::

::: exercise
De Morgan's law also implies that $\overline{A} \cap \overline{B} = \overline{A \cup B}$.
Explain why!

*Hint*: What is the complement of $\overline{A}$ relative to $U$?

::: solution
For the sake of clarity, let us rewrite De Morgan's law using $X$ and $Y$ instead of $A$ and $B$, respectively:
$X \cap Y = \overline{\overline{X} \cup \overline{Y}}$.
Hence we can calculate $\overline{A} \cap \overline{B}$ by taking $\overline{\overline{X} \cup \overline{Y}}$ and replacing $X$ with $\overline{A}$ and $Y$ with $\overline{B}$.
This yields $\overline{\overline{\overline{A}} \cup \overline{\overline{B}}}$.
But since the complement of the complement of a set is just the set itself, this reduces to $\overline{A \cup B}$, and hence we have $\overline{A} \cap \overline{B} = \overline{A \cup B}$.
:::
:::

De Morgan's law is very important and surprisingly general.
It generalizes beyond sets to logic and certain types of algebras, where it allows for some very nifty tricks.
It can also greatly simplify proofs.
But how does De Morgan help us understand that $L(G_1) \cap L(G_2) = L(G_1 \cap G_2)$ when $G_1$ and $G_2$ are positive grammars?

Well, it's because positive and negative grammars are connected by relative complement, as we already know.
Given a $n$-gram grammar $G$ of some polarity, $\Sigma_E^n - G$, or simply $\overline{G}$, is the equivalent grammar of opposite polarity.
This is all we need to derive the theorem for positive grammars from the one for negative ones.

Suppose $P_1$ and $P_2$ are positive grammars.
Then $N_1 \is \Sigma_E^n - P_1 = \overline{P_1}$ is the negative grammar counterpart to $P_1$.
And $N_2 \is \Sigma_E^n - P_2 = \overline{P_2}$ is the negative grammar counterpart to $P_2$.

::: exercise
Suppose our alphabet contains only $a$ and that $G$ is a positive grammar containing the bigrams *{{{L}}}a*, *aa*, and *a{{{R}}}* (and nothing else).
Compute $\overline{G}$, then verify for yourself that $\overline{G}$, when interpreted as a negative grammar, accepts the same strings as $G$. 

::: solution
Since all symbols must be *a*, $\Sigma_E^2$ contains all of the following:
${{{L}}}{{{R}}}$,
${{{L}}}a$,
$aa$,
$a{{{R}}}$.
The other bigrams are ${{{R}}}{{{L}}}$, $a{{{L}}}$, and ${{{R}}}a$, which are all useless and can be ignored.
Hence $\overline{G} = \Sigma_E^2 - G = {{{L}}}{{{R}}}$.
The negative grammar containing only ${{{L}}}{{{R}}}$ generates all strings over *a* except the empty string, i.e. all strings over *a* that contain 1 or more instances of *a*.
But this is exactly the language generated by the positive grammar that contains *{{{L}}}a*, *aa*, and *a{{{R}}}*.
:::
:::

So given some grammar $G$ of one polarity, $\overline{G}$ of the opposite polarity generates the same string language.
For the sake of succinctness, let us write $P(G)$ to indicate that $G$ is interpreted as a positive grammar, and $N(G)$ when $G$ is interpreted as a negative grammar.
Note that we can use this notation even if $G$ is itself defined via a more complex expression.
For instance, $P(G_1 \cup G_2)$ can be polarity converted to the equivalent $N(\overline{G_1 \cup G_2})$.

Given all these equivalences, plus De Morgan's law, we can derive the fact that $L(P(G_1)) \cap L(P(G_2)) = L(P(G1 \cap G2))$ from the previously established fact that $L(N(G_1)) \cap L(N(G_2)) = L(N(G_1 \cup G_2))$.
The sequence of steps is as follows:

1. **Starting point**: $L(P(G_1)) \cap L(P(G_2))$
1. **Convert each grammar from positive to negative**: $L(N(\overline{G_1})) \cap L(N(\overline{G_2}))$
1. **Theorem for combining negative grammars**: $L(N(\overline{G_1} \cup \overline{G_2}))$
1. **De Morgan's Law**: $L(N(\overline{G_1 \cap G_2}))$
1. **Convert from negative to positive**: $L(P(G_1 \cap G_2))$

You might have to read this several times before it starts to make sense to you.
But that's okay, math often takes some time before it clicks.
And if you still can't work your way through it after half an hour, just put it aside for now and come back later when you're more comfortable with arguments of this sort.


## Recap

- $n$-gram grammars can be combined in an automatic fashion.
  If each component of a system can be described by an $n$-gram grammar, the whole system can be described by an $n$-gram grammar.
- Negative grammars are combined via union.
- Positive grammars are combined via intersection.
- The duality between negative grammars and union on the one hand and positive grammars and intersection on the other follows from **De Morgan's law**: $A \cap B = \overline{\overline{A} \cup \overline{B}}$ (or equivalently, $\overline{A} \cap \overline{B} = \overline{A \cup B}$)

\includecollection{solutions}
