<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>An n-gram model of text</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="/main/style.css" />
  <!-- Include this in HTML headers to configure and activate MathJax. -->
  <script>
  MathJax = {
      loader: {
          load: ['a11y/assistive-mml']
      },
      options: {
          enableMenu: true,          // set to false to disable the menu
          menuOptions: {
              settings: {
                  assistiveMml: true,   // true to enable assitive MathML
              }
          }
      }
  };
  </script>
  <script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</head>
<body>
<div class="container with-sidebar">
<div class="sidenav">
<nav id="TOC" role="doc-toc">
<p><a id="site-title" href="/main">Mathematical Methods in Linguistics</a></p>
<ul>
<li><a href="#an-n-gram-model-of-text">An <span
class="math inline">\(n\)</span>-gram model of text</a>
<ul>
<li><a href="#unigrams-for-text-classification">Unigrams for text
classification</a></li>
<li><a href="#unigrams-the-pros-and-cons">Unigrams: the pros and
cons</a></li>
</ul></li>
</ul>
</nav>
</div>
<div class="content">
<h1 id="an-n-gram-model-of-text">An <span
class="math inline">\(n\)</span>-gram model of text</h1>
<div class="prereqs">
<ul>
<li>sets (cardinality)</li>
</ul>
</div>
<p>So far we have mostly studied <span
class="math inline">\(n\)</span>-gram models for linguistic reasons.
These models are very simple, but can capture a fair amount of
phonotactic and morphotactic conditions. This in turn shows that these
conditions are very simple. But <span
class="math inline">\(n\)</span>-gram models aren’t limited to
linguistic theorizing. In fact, they are mostly used in more applied
domains.</p>
<h2 id="unigrams-for-text-classification">Unigrams for text
classification</h2>
<p>Suppose your task is to classify texts, for example as part of a
search engine. Ideally, this classification would proceed by carefully
reading the entire text, interpreting it, and distilling its core themes
through some high-level analysis. But that requires a lot of time and
skill, and may simply not be feasible in practice. How does one
adequately summarize, say, the 1130 pages of Robert Musil’s <a
href="https://en.wikipedia.org/wiki/The_Man_Without_Qualities"><em>The
Man Without Qualities</em></a>, or Grigori Perelman’s proof of the <a
href="https://en.wikipedia.org/wiki/Poincar%C3%A9_conjecture">Poincaré
conjecture</a>? Whatever the right answer, it probably isn’t something
that can be done quickly and automatically. And while one may be able to
pay experts to work on these singular accomplishments, it’s much harder
to find somebody to summarize papers on cell biology because there are
so many published every day. With internet websites, human summarization
is completely impossible given how often they are updated and how many
new ones are created every minutes. So instead computers have to do the
job, and since we haven’t figured out a way yet to get computers to
understand text, the models are necessarily simple and focussed on
surface features. Virtually all of them build on <span
class="math inline">\(n\)</span>-grams, the core idea being that the
meaning of a text can be equated with the words that occur in it.</p>
<p>Let us look at a particularly simple way of formalizing this idea,
one where we ignore how often certain words occur. We will also ignore
capitalization, as is commonly done in this model. For example,
converting the mini-text <em>Only John could like John</em> to a set of
unigrams (i.e. <span class="math inline">\(1\)</span>-grams) only
preserves the information that the text contains the words
<em>only</em>, <em>john</em>, <em>could</em>, and <em>like</em>. A few
more examples are shown below using the programming language Python.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pprint <span class="im">import</span> pprint</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> set_of_words(string):</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Convert a string to a set of words.&quot;&quot;&quot;</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    tokens <span class="op">=</span> [word <span class="cf">for</span> word <span class="kw">in</span> re.split(<span class="st">&quot;\W&quot;</span>, string.lower()) <span class="cf">if</span> word]</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;Input:&quot;</span>, string)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    pprint(<span class="bu">set</span>(tokens))</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> set_of_words(<span class="st">&quot;John is John, that much is obvious!&quot;</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>Input: John <span class="kw">is</span> John, that much <span class="kw">is</span> obvious<span class="op">!</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>{<span class="st">&#39;is&#39;</span>, <span class="st">&#39;obvious&#39;</span>, <span class="st">&#39;john&#39;</span>, <span class="st">&#39;much&#39;</span>, <span class="st">&#39;that&#39;</span>}</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> set_of_words(<span class="st">&quot;The man and the woman are husband and wife.&quot;</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>Input: The man <span class="kw">and</span> the woman are husband <span class="kw">and</span> wife.</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>{<span class="st">&#39;wife&#39;</span>, <span class="st">&#39;the&#39;</span>, <span class="st">&#39;and&#39;</span>, <span class="st">&#39;husband&#39;</span>, <span class="st">&#39;woman&#39;</span>, <span class="st">&#39;are&#39;</span>, <span class="st">&#39;man&#39;</span>}</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> set_of_words(<span class="st">&quot;Police police police police police.&quot;</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>Input: Police police police police police.</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>{<span class="st">&#39;police&#39;</span>}</span></code></pre></div>
<p>A search engine, for instance, could use this model to convert any
given website to a set of words. When the user enters a query,
e.g. <em>fed my Gremlin after midnight</em>, the search engine could
convert the query to a set of words, too, and then check which websites
have a similar set of words.</p>
<div class="example">
<p>Suppose that our search engine has only indexed three websites so
far, both of which contain very little text.</p>
<ul>
<li>This website is a website about the movie Gremlins.</li>
<li>I never feed my cat after midnight.</li>
<li>There’s a Gremlin in my closet.</li>
</ul>
<p>The corresponding sets of unigrams are as follows:</p>
<ul>
<li><span class="math inline">\(\left \{ \text{this}, \text{website},
\text{is}, \text{a}, \text{about}, \text{the}, \text{movie},
\text{gremlins} \right \}\)</span></li>
<li><span class="math inline">\(\left \{ \text{i}, \text{never},
\text{feed}, \text{my}, \text{cat}, \text{after}, \text{midnight} \right
\}\)</span></li>
<li><span class="math inline">\(\left \{ \text{there&#39;s}, \text{a},
\text{gremlin}, \text{in}, \text{my}, \text{closet} \right
\}\)</span></li>
</ul>
<p>The query <em>fed my Gremlin after midnight</em> is mapped to the set
<span class="math inline">\(\left \{ \text{fed}, \text{my},
\text{gremlin}, \text{after}, \text{midnight} \right \}\)</span>. We can
now use intersection to see how much each website overlaps with the
query.</p>
<ul>
<li><span class="math inline">\(\emptyset\)</span></li>
<li><span class="math inline">\(\left \{ \text{my}, \text{after},
\text{midnight} \right \}\)</span></li>
<li><span class="math inline">\(\left \{ \text{gremlin}, \text{my}
\right \}\)</span></li>
</ul>
<p>The largest overlap is with the second website— or in more
mathematical terms, the intersection of our query with the second
website has the largest cardinality.</p>
</div>
<div class="exercise">
<p>This model is hopelessly olibvious about the connections between
words. Give two examples of important connections that this model
missed. Would this have changed which website is a better fit?</p>
</div>
<p>The general idea of the model is simple enough, and as you can see
even the implementation in a programming language is straight-forward.
Note that here we are no longer dealing with <span
class="math inline">\(n\)</span>-gram grammars. The task involves no
notion of well-formedness. Instead, unigrams are used as a compressed
<strong>representation</strong> of the text, and all reasoning is done
over this compressed representation.</p>
<h2 id="unigrams-the-pros-and-cons">Unigrams: the pros and cons</h2>
<p>The main advantage of the set-of-words model of texts is its
simplicity — determining the meaning of a text only requires a very
simple function that maps strings of words to sets of words. But while
practical applications prize simplicity and efficiency even to the
detriment of accuracy, the set-of-words model is just too simple for the
vast majority of real-world applications. There are at least three
problems:</p>
<ol type="1">
<li>Context is not taken into account at all, even within individual
sentences. Among other things, <em>The dog bit the man</em> and <em>The
man bit the dog</em> incorrectly receive the same meaning. And along the
same lines, <em>Not every student thinks they should leave</em> and
<em>Every student thinks they should not leave</em> are taken to have
identical meanings, too.</li>
<li>Since we do not count how often words occur, a text that mentions
global warming once in passing is taken to cover this topic to the same
extent as one that mentions it over a hundred times.</li>
<li>The sets are cluttered with uninformative words like <em>is</em>,
<em>the</em>, <em>of</em>, and so on.</li>
</ol>
<p>The first one can be improved by moving from unigrams to <span
class="math inline">\(n\)</span>-grams. With bigrams, a headline like
<em>man bites dogs</em> is represented as the set <span
class="math inline">\(\left \{ \text{man bites}, \text{bites dogs}
\right \}\)</span>, whereas the much less startling <em>dog bites
man</em> becomes <span class="math inline">\(\left \{ \text{dog bites},
\text{bites man} \right \}\)</span>. Note that we could also include our
edge markers &gt; and &lt; to clearly identify the first and last word
of a sentence. None of this is an adequate representation of context,
but it nonetheless works fairly well in practice - though that might
just be because the practical problems language technology is asked to
solve nowadays are still fairly simple.</p>
<p>Be that as it may, there are still problems 2 and 3 to take care of.
We need a way to keep track of how often specific words occurred, and we
want to get rid of uninformative words. Those will require a few bells
and whistles we haven’t seen before, which will be the subject of the
next few units.</p>
</div>
</div>
</body>
</html>
