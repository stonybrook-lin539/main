<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>mathcommands-preproc</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="/main/style.css" />
  <!-- Include this in HTML headers to configure and activate MathJax. -->
  <script>
  MathJax = {
      loader: {
          load: ['a11y/assistive-mml']
      }
      options: {
          menuOptions: {
              settings: {
                  assistiveMml: true,   // true to enable assitive MathML
              }
          }
      }
  };
  </script>
  <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"></script>
</head>
<body>
<div class="container with-sidebar">
<div class="sidenav">
<nav id="TOC" role="doc-toc">
<p><a id="site-title" href="/main">Mathematical Methods in Linguistics</a></p>
<ul>
<li><a href="#dealing-with-stop-words">Dealing with stop words</a>
<ul>
<li><a href="#stop-words-the-intuition">Stop words: The
intuition</a></li>
<li><a href="#stop-word-removal-as-a-function">Stop word removal as a
function</a></li>
<li><a href="#mathematical-functions">Mathematical functions</a></li>
<li><a href="#recap">Recap</a></li>
</ul></li>
</ul>
</nav>
</div>
<div class="content">
<h1 id="dealing-with-stop-words">Dealing with stop words</h1>
<h2 id="stop-words-the-intuition">Stop words: The intuition</h2>
<p>The bag of words model can be used for a variety of tasks. In fact,
it is part of almost every natural language model in the industry: web
search, word prediction, spell checking, machine translation, chatbots,
you name it. None of these applications count all the words in the text.
Instead, many words are ignored, just like we only used counts for a few
words like <em>Aristotle</em>, <em>Plato</em>, <em>ethics</em>, and
<em>metaphysics</em> in the previous notebook. The reason is simple: if
we kept track of the counts for every single word, then interesting
words like <em>Plato</em> and <em>ethics</em> would be completely
drowned out by irrelevant high-frequency words like <em>the</em> and
<em>is</em>.</p>
<div class="example">
<p>In the previous notebook we had counts for four websites <span
class="math inline">\(w_1\)</span>, <span
class="math inline">\(w_2\)</span>, <span
class="math inline">\(w_3\)</span> and <span
class="math inline">\(w_4\)</span>, as shown below. <span
class="math display">\[
\begin{array}{rl}
    w_1 &amp; \mathrel{\mathop:}=\left \{ \text{Aristotle}: 9,
\text{ethics}: 3, \text{metaphysics}: 0, \text{Plato}: 6,
\text{Sokrates}: 0 \right \}\\
    w_2 &amp; \mathrel{\mathop:}=\left \{ \text{Aristotle}: 1,
\text{ethics}: 0, \text{metaphysics}: 9, \text{Plato}: 5,
\text{Sokrates}: 0 \right \}\\
    w_3 &amp; \mathrel{\mathop:}=\left \{ \text{Aristotle}: 4,
\text{ethics}: 9, \text{metaphysics}: 5, \text{Plato}: 8,
\text{Sokrates}: 2 \right \}\\
    w_4 &amp; \mathrel{\mathop:}=\left \{ \text{Aristotle}: 0,
\text{ethics}: 7, \text{metaphysics}: 3, \text{Plato}: 9,
\text{Sokrates}: 0 \right \}\\
\end{array}
\]</span> With these counts, the best match for the query <em>Aristotle
metaphysics</em> is <span class="math inline">\(w_1\)</span> (assuming
that the first word of the query is 50% more important than the second
one). Now suppose that the query is <em>Aristotle’s metaphysics</em>,
which we analyze as consisting of three words: <em>Aristotle</em>, the
possessive marker <em>’s</em>, and <em>metaphysics</em>. Since we have
no counts for the possessive marker, the results don’t change and <span
class="math inline">\(w_1\)</span> is still the best match.</p>
<p>But let’s assume that we actually included counts for <em>’s</em>,
yielding the following multisets: <span class="math display">\[
\begin{array}{rl}
    w_1 &amp; \mathrel{\mathop:}=\left \{ \text{Aristotle}: 9,
\text{ethics}: 3, \text{metaphysics}: 0, \text{Plato}: 6,
\text{Sokrates}: 0,
    \text{&#39;s}: 20 \right \}\\
    w_2 &amp; \mathrel{\mathop:}=\left \{ \text{Aristotle}: 1,
\text{ethics}: 0, \text{metaphysics}: 9, \text{Plato}: 5,
\text{Sokrates}: 0,
    \text{&#39;s}: 27 \right \}\\
    w_3 &amp; \mathrel{\mathop:}=\left \{ \text{Aristotle}: 4,
\text{ethics}: 9, \text{metaphysics}: 5, \text{Plato}: 8,
\text{Sokrates}: 2,
    \text{&#39;s}: 2 \right \}\\
    w_4 &amp; \mathrel{\mathop:}=\left \{ \text{Aristotle}: 0,
\text{ethics}: 7, \text{metaphysics}: 3, \text{Plato}: 9,
\text{Sokrates}: 0,
    \text{&#39;s}: 86 \right \}\\
\end{array}
\]</span> The corresponding multisets for the words in the query are
<span class="math inline">\(_MA\)</span> for <em>Aristotle</em>, <span
class="math inline">\(_MP\)</span> for <em>metaphysics</em>, and <span
class="math inline">\(_MS\)</span> for the possessive marker. <span
class="math display">\[
\begin{array}{rl}
    _MA &amp; \mathrel{\mathop:}=\left \{ w_1: 9, w_2: 1, w_3: 4, w_4: 0
\right \}\\
    _MP &amp; \mathrel{\mathop:}=\left \{ w_1: 0, w_2: 9, w_3: 5, w_4: 3
\right \}\\
    _MS &amp; \mathrel{\mathop:}=\left \{ w_1: 20, w_2: 27, w_3: 2, w_4:
86 \right \}\\
\end{array}
\]</span> Let’s assume a weighting where the counts for the <span
class="math inline">\(n\)</span>-th query are multiplied by <span
class="math inline">\(\frac{1}{n}\)</span> (rounded down).</p>
<p><span class="math display">\[\begin{align*}
    1 \otimes_MA \uplus
    \frac{1}{2} \otimes_MS \uplus
    \frac{1}{3} \otimes_MP
    =&amp;
    \left \{ w_1: 9, w_2: 1, w_3: 4, w_4: 0 \right \}
    \uplus\\
    &amp;
    \left \{ w_1:10, w_2: 13, w_3: 1, w_4: 43 \right \}
    \uplus\\
    &amp;
    \left \{ w_1: 0, w_2: 3, w_3: 1, w_4: 1 \right \}
    \\
    =&amp;
    \left \{ w_1: 19, w_2: 17, w_3: 6, w_4: 44 \right \}
\end{align*}\]</span></p>
<p>As you can see, <span class="math inline">\(w_4\)</span> is now the
best match, even though it does not contain a single mention of
<em>Aristotle</em>, the most important word in the search query.</p>
</div>
<p>This example shows that not all words are on equal footing as far as
the bag of words model is concerned. Some contribute useful information,
whereas others are pretty useless. At the best, they add unnecessary
clutter to the set, at worst, they make the model worse.</p>
<p>The problem of cluttered sets is a very real one. This is due to
<strong>Zipf’s law</strong>, which was formulated by the linguist <a
href="https://en.wikipedia.org/wiki/George_Kingsley_Zipf">George
Kingsley Zipf</a>.</p>
<div class="definition" name="Zipf&#39;s law">
<p>Let <span class="math inline">\(t\)</span> be a text of natural
language utterances, and <span class="math inline">\(R\)</span> a linear
ranking of words by their frequency in <span
class="math inline">\(t\)</span>. Then the frequency <span
class="math inline">\(f\)</span> of a word in <span
class="math inline">\(t\)</span> is inversely proportional to its rank
<span class="math inline">\(r\)</span> in <span
class="math inline">\(R\)</span> (<span class="math inline">\(f =
\dfrac{1}{r}\)</span>).</p>
</div>
<p>Zipf’s law tell us that the most common word in a text occurs</p>
<ul>
<li>two times as often as the second most common one,</li>
<li>three times as often as the third most common one,</li>
<li>four times as often as the fourth most common one,</li>
<li>and so on.</li>
</ul>
<p>So if the most common word occurs 100 times, the second occurs 50
times, the third one 33 times, the fourth one 25 times, and so on. As in
all statistical claims, there is a certain margin of error, so the
second word may actually occur 57 times and the third one 26 times. It
also happens quite a lot that multiple word have approximately the same
rank. This means that the most common word may occur 100 times, the next
three each around 50 times, and then the next two 33 times. In this
case, Zipf’s law still holds across ranks, but each rank may correspond
to multiple words.</p>
<p>These minor details do not change the fact, though, that the
frequency distribution of a natural language will have a very peculiar
shape. A few word types make up the majority of word tokens in a text
while many other word types only have one token each.</p>
<p>If we draw a plot with the word types in a text in descending
frequency along the <span class="math inline">\(x\)</span>-axis and
their number of tokens as the <span
class="math inline">\(y\)</span>-axis, this produces a curve that starts
out high, drops off quickly, and then has a very long tail that
approaches 0 but never reaches it.</p>
<div class="exercise">
<p>Suppose you have a text with 20,000 words where the most frequent
word occurs 1068 times. Assuming a distribution that is exactly Zipfian,
calculate the frequencies for rank 2, 3, 5, 15, 50, 100, 200, 500, and
1000. Give a rough estimate how many of the top ranks jointly make up at
least 50% of the text.</p>
</div>
<p>Zipf’s law is more than an interesting curiosity, it has noticeable
repercussions in practice. In fact, it is one of the primary reasons why
language technology is still much less capable than humans despite the
recent advances in machine learning. Machine learning algorithms are
designed to focus on the robust, very frequent aspects of a data set
while ignoring the rare outliers. But Zipf’s law tells us that this is
exactly the wrong approach for language. Here is what you get if you
take the first 250 words of <em>Moby Dick</em> and throw away any words
that aren’t among the 250 most common in the book.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> urllib.request</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> analyze_mobydick():</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    urllib.request.urlretrieve(<span class="st">&quot;https://www.gutenberg.org/files/2701/2701-0.txt&quot;</span>,</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>                               <span class="st">&quot;mobydick.txt&quot;</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(<span class="st">&quot;mobydick.txt&quot;</span>, <span class="st">&quot;r&quot;</span>) <span class="im">as</span> text:</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># tokenize string into list of lowercase words</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> re.split(<span class="st">&quot;\W&quot;</span>, text.read().lower())</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># compute most frequent word types</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>        most_frequent <span class="op">=</span> [count[<span class="dv">0</span>] <span class="cf">for</span> count <span class="kw">in</span> Counter(text).most_common(<span class="dv">250</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>                         <span class="cf">if</span> count[<span class="dv">0</span>]]</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># find the beginning of Chapter 1 in the file</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> n <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(text)):</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> text[n] <span class="op">==</span> <span class="st">&quot;call&quot;</span> <span class="kw">and</span><span class="op">\</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>            text[n<span class="op">+</span><span class="dv">1</span>] <span class="op">==</span> <span class="st">&quot;me&quot;</span> <span class="kw">and</span><span class="op">\</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>            text[n<span class="op">+</span><span class="dv">2</span>] <span class="op">==</span> <span class="st">&quot;ishmael&quot;</span>:</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>                start <span class="op">=</span> n</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>            <span class="co"># and now take the first 250 words and remove all infrequent ones</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>            filtered <span class="op">=</span> [word <span class="cf">for</span> word <span class="kw">in</span> text[n:n<span class="op">+</span><span class="dv">250</span>]</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">if</span> word <span class="kw">in</span> most_frequent]</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>            <span class="co"># and put the tokenized list back into a string</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">&quot; &quot;</span>.join(filtered))</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span>:</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">&quot;Couldn&#39;t find start&quot;</span>)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> analyze_mobydick()</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>me some years never how <span class="bu">long</span> little <span class="kw">or</span> no <span class="kw">in</span> my <span class="kw">and</span> nothing to me on i thought i would sail about a little <span class="kw">and</span> see the part of the world it <span class="kw">is</span> a way i have of off the <span class="kw">and</span> the i about the it <span class="kw">is</span> a <span class="kw">in</span> my i before <span class="kw">and</span> up the of every i <span class="kw">and</span> my such an hand of me that it a to me <span class="im">from</span> into the <span class="kw">and</span> s off then i it high time to to sea <span class="im">as</span> soon <span class="im">as</span> i can this <span class="kw">is</span> my <span class="cf">for</span> <span class="kw">and</span> <span class="cf">with</span> a himself upon his i take to the ship there <span class="kw">is</span> nothing <span class="kw">in</span> this <span class="cf">if</span> they but it almost <span class="bu">all</span> men <span class="kw">in</span> their some time <span class="kw">or</span> other very the same towards the <span class="cf">with</span> me there now <span class="kw">is</span> your of the <span class="bu">round</span> by <span class="im">as</span> by it <span class="cf">with</span> her</span></code></pre></div>
<p>Not too illuminating, is it? With language, it is the outliers that
are interesting rather than the very frequent stuff. An algorithm that
favors the excessively common over the rare is ill-suited for
language.</p>
<p>Alright, let us return to our original bag of words model and improve
it by removing uninformative word types from the unigram set. First we
must agree on what counts as an uninformative word type. For the sake of
simplicity, we will consider a word type uninformative iff it is a
<strong>stop word</strong>, i.e. a word type with a very high token
frequency.</p>
<p>Here is a list of English stop words (using a very lenient definition
of word that also includes contractions):</p>
<blockquote>
<p>a, about, above, after, again, against, all, am, an, and, any, are,
aren’t, as, at, be, because, been, before, being, below, between, both,
but, by, can’t, cannot, could, couldn’t, did, didn’t, do, does, doesn’t,
doing, don’t, down, during, each, few, for, from, further, had, hadn’t,
has, hasn’t, have, haven’t, having, he, he’d, he’ll, he’s, her, here,
here’s, hers, herself, him, himself, his, how, how’s, i, i’d, i’ll, i’m,
i’ve, if, in, into, is, isn’t, it, it’s, its, itself, let’s, me, more,
most, mustn’t, my, myself, no, nor, not, of, off, on, once, only, or,
other, ought, our, ours, ourselves, out, over, own, same, shan’t, she,
she’d, she’ll, she’s, should, shouldn’t, so, some, such, than, that,
that’s, the, their, theirs, them, themselves, then, there, there’s,
these, they, they’d, they’ll, they’re, they’ve, this, those, through,
to, too, under, until, up, very, was, wasn’t, we, we’d, we’ll, we’re,
we’ve, were, weren’t, what, what’s, when, when’s, where, where’s, which,
while, who, who’s, whom, why, why’s, with, won’t, would, wouldn’t, you,
you’d, you’ll, you’re, you’ve, your, yours, yourself, yourselves</p>
</blockquote>
<p>Given such a list of stop words, it is a simple task to remove all
stop words from a text.</p>
<div class="example">
<p>Here is a very short mini-text, the first sentence of Samuel
Beckett’s novel <em>Murphy</em>: <em>The sun shone, having no
alternative, on the nothing new</em>. We can then go through the text
word by word and delete each word token that is on our list of stop
words. We ignore capitalization, but treat punctuation as separate
words.</p>
<ul>
<li><em>the</em>: delete</li>
<li><em>sun</em>: keep</li>
<li><em>shone</em>: keep</li>
<li><em>,</em>: keep</li>
<li><em>having</em>: delete</li>
<li><em>no</em>: delete</li>
<li><em>alternative</em>: keep</li>
<li><em>,</em>: keep</li>
<li><em>on</em>: delete</li>
<li><em>the</em>: delete</li>
<li><em>nothing</em>: keep</li>
<li><em>new</em>: keep</li>
<li><em>.</em>: keep</li>
</ul>
<p>After stop word removal, the sentence has been truncated to <em>sun
shone, alternative, nothing new</em>. Clearly the removal of stop words
has completely destroyed the meaning of the sentence for humans. But a
bag of words model can’t capture meaning at this abstract level anyways,
it can at best give a rough estimate as to what topics a text is about.
For this limited understanding of text, removing stop words improves
results.</p>
</div>
<div class="exercise">
<p>Igoring capitalization, what do the following strings look like after
stop word removal?</p>
<ul>
<li>Hi! What’s up?</li>
<li>How much wood would a woodchuck chuck if a woodchuck could chuck
wood?</li>
<li>I’m here to kick ass and chew bubblegum, and I’m all out of
gum.</li>
<li>There is no spoon.</li>
<li>John loves Peter.</li>
<li>Every student hates some professor.</li>
<li>Some student hates some professor.</li>
</ul>
<p>Are there any sentences whose original meaning can still inferred
after stop word removal? If so, why might that be?</p>
</div>
<h2 id="stop-word-removal-as-a-function">Stop word removal as a
function</h2>
<p>If you were to implement stop word removal in a programming language,
say Python, you’d probably write it as a <strong>function</strong>.</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> remove_stopwords(text, stopwords):</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    text_list <span class="op">=</span> re.findall(<span class="vs">r&quot;\w+&quot;</span>, text)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="st">&#39; &#39;</span>.join([word <span class="cf">for</span> word <span class="kw">in</span> text_list</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>                     <span class="cf">if</span> word <span class="kw">not</span> <span class="kw">in</span> stopwords])</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>stopwords <span class="op">=</span> [<span class="st">&quot;a&quot;</span>, <span class="st">&quot;the&quot;</span>, <span class="st">&quot;have&quot;</span>, <span class="st">&quot;having&quot;</span>, <span class="st">&quot;no&quot;</span>, <span class="st">&quot;not&quot;</span>, <span class="st">&quot;on&quot;</span>, <span class="st">&quot;in&quot;</span>]</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">&quot;the sun shone, having no alternative, on the nothing new.&quot;</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(remove_stopwords(text, stopwords))</span></code></pre></div>
<p>A function is like an abstract machine that takes one or more
<strong>arguments</strong> as input and combines and alters them in some
way to produce a unique output. In the code above, the function
<code>remove_stopwords</code></p>
<ul>
<li>takes two inputs
<ol type="1">
<li>a string as its first argument (called <code>text</code> in the
code)</li>
<li>a list of stop words as its second argument</li>
</ol></li>
<li>produces as its output a new string, which is the result of taking
the first argument and removing all stop words (and all punctuation) as
specified by the second argument.</li>
</ul>
<p>The Python code is pretty cumbersome to read because it has to take
care of all kinds of programming details. For instance, the string first
has to be converted to a list of words, and then this list has to be
reassembled into a string after stop words have been removed. The
mathematical perspective can abstract away from these details to define
functions in much simpler terms.</p>
<h2 id="mathematical-functions">Mathematical functions</h2>
<p>Mathematics has specialized notation to talk about functions, but
let’s put that aside for now. The most important fact about functions is
that they can be so much more than what you’ve seen in your high school
class. Yes, <span class="math inline">\(f(x) = 2 \times x\)</span> is a
function that takes a number as its only argument and doubles it. But so
many more things are functions. A function is any kind of system that
takes a fixed number of inputs and always returns the same output for
the same input.</p>
<div class="example">
<p>Consider <span class="math inline">\(f(x) = x\)</span>. This is a
function: it takes a fixed number of arguments (i.e. one) and the value
of <span class="math inline">\(f(x)\)</span> stays the same as long as
<span class="math inline">\(x\)</span> does not change.</p>
</div>
<div class="example">
<p>Another mathematical function is <span class="math inline">\(f(x) =
1\)</span>. This is a constant function, which means that all arguments
yield the same output.</p>
</div>
<div class="example">
<p>Each <span class="math inline">\(n\)</span>-gram grammar can be
regarded as a function that maps strings to 1 (= well-formed) or 0 (=
ill-formed). Given a fixed grammar <span
class="math inline">\(G\)</span>, a string <span
class="math inline">\(s\)</span> is either well-formed or ill-formed,
but never both. So <span class="math inline">\(G\)</span> assigns a
unique value to <span class="math inline">\(s\)</span> that never
changes.</p>
<p>The functional perspective of <span
class="math inline">\(n\)</span>-gram grammars will be very useful later
on when we consider probabilistic version of n-gram grammars.</p>
</div>
<div class="example">
<p>A dispersive prism breaks light down into its spectral colors. It
thus is a function that takes light as input and produces a unique
spectral decomposition as its output.</p>
</div>
<div class="example">
<p>A soda machine is a function that takes two inputs: a fixed monetary
value (usually fed in piecewise as coins) and a number that encodes your
choice of beverage. If everything is in order, it outputs the desired
beverage. If something is wrong with the input, you don’t get
anything.</p>
</div>
<div class="exercise">
<p>A soda machine corresponds to different functions depending on</p>
<ul>
<li>how much change it has,</li>
<li>which beverages are in stock.</li>
</ul>
<p>Explain why.</p>
</div>
<div class="example">
<p>Consider a program that takes as input a collection of words and
returns a randomly generated sentence using those words. For instance,
if you give it the words <em>Mary</em>, <em>ate</em>, and
<em>sushi</em>, it may return</p>
<ul>
<li><em>Mary ate sushi</em>, or</li>
<li><em>sushi at Mary</em>, or</li>
<li><em>sushi, Mary ate</em>, or</li>
<li><em>Mary, sushi ate</em>.</li>
</ul>
<p>This is not a function because one and the same input can result in
different outputs.</p>
</div>
<div class="exercise">
<p>For each one of the following, say whether or not it is a function
(i.e. it takes a fixed number of arguments and no input can yield more
than one output). If things can go either way depending on additional
assumptions, explain why. Whether your answer is correct is less
important than whether you can justify it based on your assumptions
about how the world works.</p>
<ul>
<li><span class="math inline">\(f(x) = x^2 - x\)</span></li>
<li>an address book that only contains names and phone numbers</li>
<li>a spam filter for emails</li>
<li>chess</li>
<li><span class="math inline">\(\in\)</span></li>
<li>the US laws for assigning children to schools</li>
</ul>
</div>
<p>Given this very general understanding of functions, we can define
stop word removal as a function <span
class="math inline">\(\mathit{del}\)</span> that takes as its input a
string <span class="math inline">\(s\)</span> and a set <span
class="math inline">\(S\)</span> of stop words. Then the output <span
class="math inline">\(\mathit{del}(s,S)\)</span> is the unique string
that is obtained from <span class="math inline">\(s\)</span> by deleting
every symbol that is a member of <span class="math inline">\(S\)</span>.
And that’s pretty much all that needs to be said.</p>
<p>How exactly this deletion is carried out is immaterial here. We could
process <span class="math inline">\(s\)</span> from left-to-right,
right-to-left, inside out, or do something more complex like in the
Python code above. None of these distinctions affect what output is
produced by <span class="math inline">\(\mathit{del}(s,S)\)</span>. This
is the strength of a mathematical definition of a function. It cuts out
the procedural details to clearly state the connection between inputs
and outputs.</p>
<p>You might be flummoxed, though, that we defined <span
class="math inline">\(\mathit{del}(s,S)\)</span> is plain English. All
the functions you know from school were probably stated in terms of
equations, e.g. <span class="math inline">\(f(x,y,z) = x^y - 5 \times
z\)</span>. This isn’t a necessity. Sometimes a function can be stated
clearly enough without special notation. But for the curious among you,
the next notebook gives a more formal definition of <span
class="math inline">\(\mathit{del}(s,S)\)</span>.</p>
<p>You might also be wondering what we gain from defining stop word
removal in mathematical terms. The notebook after the next one
illustrates how a more abstract understanding in terms of mathematical
functions can reveal surprising parallels between problems that seem
entirely unrelated.</p>
<h2 id="recap">Recap</h2>
<ul>
<li>Many aspects of language exhibit a Zipfian distribution.</li>
</ul>
<div class="definition">
<p>Let <span class="math inline">\(t\)</span> be a text of natural
language utterances, and <span class="math inline">\(R\)</span> a linear
ranking of words by their frequency in <span
class="math inline">\(t\)</span>. Then the frequency <span
class="math inline">\(f\)</span> of a word in <span
class="math inline">\(t\)</span> is inversely proportional to its rank
<span class="math inline">\(r\)</span> in <span
class="math inline">\(R\)</span> (<span class="math inline">\(f =
\frac{1}{r}\)</span>).</p>
</div>
<ul>
<li><p>Over 50% of any natural language text are made up from a few very
high frequency words like <em>a</em>, <em>the</em>, <em>is</em>,
<em>of</em>, and so on.</p></li>
<li><p>These <strong>stop words</strong> must be removed in order for
bag of words models to perform well.</p></li>
<li><p>Stop word removal can be regarded as a mathematical function that
takes as its input a string <span class="math inline">\(s\)</span> and a
set <span class="math inline">\(S\)</span> of symbols and returns a
string that is identical to <span class="math inline">\(s\)</span>
except that all symbols that belong to <span
class="math inline">\(S\)</span> have been deleted.</p></li>
</ul>
</div>
</div>
</body>
</html>
