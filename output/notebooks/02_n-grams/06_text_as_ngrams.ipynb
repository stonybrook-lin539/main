{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\is}{\\mathrel{\\mathop:=}}$\n",
    "$\\newcommand{\\range}{\\mathop{ran}}$\n",
    "$\\newcommand{\\setof}[1]{\\left \\{ #1 \\right \\}}$\n",
    "$\\newcommand{\\card}[1]{\\left | #1 \\right |}$\n",
    "$\\newcommand{\\tuple}[1]{\\left \\langle #1 \\right \\rangle}$\n",
    "$\\newcommand{\\emptytuple}{\\left \\langle \\right \\rangle}$\n",
    "$\\newcommand{\\tuplecat}{\\cdot}$\n",
    "$\\newcommand{\\stringcat}{\\cdot}$\n",
    "$\\newcommand{\\emptystring}{\\varepsilon}$\n",
    "$\\newcommand{\\String}[1]{\\mathit{#1}}$\n",
    "$\\newcommand{\\LeftEdgeSymbol}{\\rtimes}$\n",
    "$\\newcommand{\\RightEdgeSymbol}{\\ltimes}$\n",
    "$\\newcommand{\\LeftEdge}{\\LeftEdgeSymbol}$\n",
    "$\\newcommand{\\RightEdge}{\\RightEdgeSymbol}$\n",
    "$\\newcommand{\\mult}{\\times}$\n",
    "$\\newcommand{\\multisum}{\\uplus}$\n",
    "$\\newcommand{\\multimult}{\\otimes}$\n",
    "$\\newcommand{\\freqsymbol}{\\mathrm{freq}}$\n",
    "$\\newcommand{\\freq}[1]{\\freqsymbol(#1)}$\n",
    "$\\newcommand{\\prob}{P}$\n",
    "$\\newcommand{\\counts}[2]{\\card{#2}_{#1}}$\n",
    "$\\newcommand{\\inv}[1]{#1^{-1}}$\n",
    "$\\newcommand{\\Lex}{\\mathit{Lex}}$\n",
    "$\\newcommand{\\length}[1]{\\left | #1 \\right |}$\n",
    "$\\newcommand{\\suc}{S}$\n",
    "$\\newcommand{\\sprec}{<}$\n",
    "$\\newcommand{\\Rcomp}[2]{#1 \\circ #2}$\n",
    "$\\newcommand{\\domsymbol}{\\triangleleft}$\n",
    "$\\newcommand{\\idom}{\\domsymbol}$\n",
    "$\\newcommand{\\pdom}{\\domsymbol^+}$\n",
    "$\\newcommand{\\rdom}{\\domsymbol^*}$\n",
    "$\\newcommand{\\indegree}[1]{\\mathrm{in(#1)}}$\n",
    "$\\newcommand{\\outdegree}[1]{\\mathrm{out(#1)}}$\n",
    "$\\newcommand{\\cupdot}{\\cup\\mkern-11.5mu\\cdot\\mkern5mu}$\n",
    "$\\newcommand{\\mymatrix}[1]{\\left ( \\matrix{#1} \\right )}$\n",
    "$\\newcommand{\\id}{\\mathrm{id}}$\n",
    "\n",
    "# An $n$-gram model of text\n",
    "\n",
    "So far we have mostly studied $n$-gram models for linguistic reasons.\n",
    "These models are very simple, but can capture a fair amount of phonotactic and morphotactic conditions.\n",
    "This in turn shows that these conditions are very simple.\n",
    "But $n$-gram models aren't limited to linguistic theorizing.\n",
    "In fact, they are mostly used in more applied domains.\n",
    "\n",
    "## Unigrams for text classification\n",
    "\n",
    "Suppose your task is to classify texts, for example as part of a search engine.\n",
    "Ideally, this classification would proceed by carefully reading the entire text, interpreting it, and distilling its core themes through some high-level analysis.\n",
    "But that requires a lot of time and skill, and may simply not be feasible in practice.\n",
    "How does one adequately summarize, say, the 1130 pages of Robert Musil's *The Man Without Qualities*, or Grigori Perelman's proof of the PoincarÃ© conjecture?\n",
    "Whatever the right answer, it probably isn't something that can be done quickly and automatically.\n",
    "And while one may be able to pay experts to work on these outstanding accomplishments, it's much harder to find somebody to summarize papers on cell biology because there are so many published every day.\n",
    "With internet websites, human summarization is completely impossible given how often they are updated and how many new ones are created every minutes.\n",
    "So instead computers have to do the job, and since we haven't figured out a way yet to get computers to understand text, the models are necessarily simple and focussed on surface features.\n",
    "Virtually all of them build on $n$-grams, the core idea being that the meaning of a text can be equated with the words that occur in it.\n",
    "\n",
    "Let us look at a particularly simple way of formalizing this idea, one where we ignore how often certain words occur.\n",
    "We will also ignore capitalization, as is commonly done in this model. \n",
    "For example, converting the mini-text *Only John could like John* to a set of unigrams (i.e. $1$-grams) only preserves the information that the text contains the words *only*, *john*, *could*, and *like*.\n",
    "A few more examples are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: John is John, that much is obvious!\n",
      "{'john', 'much', 'obvious', 'that', 'is'}\n",
      "\n",
      "\n",
      "Input: The man and the woman are husband and wife.\n",
      "{'and', 'wife', 'the', 'man', 'are', 'husband', 'woman'}\n",
      "\n",
      "\n",
      "Input: Police police police police police.\n",
      "{'police'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "def set_of_words(string):\n",
    "    \"\"\"convert a string to a set of words\"\"\"\n",
    "    tokens = [word for word in re.split(\"[^\\w]\", string.lower()) if word]\n",
    "    print(\"Input:\", string)\n",
    "    pprint(set(tokens))\n",
    "    print(\"\\n\")\n",
    "\n",
    "set_of_words(\"John is John, that much is obvious!\")\n",
    "set_of_words(\"The man and the woman are husband and wife.\")\n",
    "set_of_words(\"Police police police police police.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A search engine, for instance, could use this model to convert any given website to a set of words.\n",
    "When the user enters a query, e.g. *fed my Gremlin after midnight*, the search engine could convert the query to a set of words, too, and then check which websites have a similar set of words.\n",
    "\n",
    "The general idea of the model is simple enough, and as you can see even the implementation in a programming language is straight-forward.\n",
    "Note that here we are no longer dealing with $n$-gram grammars.\n",
    "The task involves no notion of well-formedness.\n",
    "Instead, unigrams are used as a compressed **representation** of the text, and all reasoning is done over this compressed representation.\n",
    "\n",
    "The main advantage of the set-of-words model of texts is its simplicity - determining the meaning of a text only requires our very simple function $b$, which maps strings of words to sets of words.\n",
    "But while practical applications often rank simplicity and efficiency over accuracy, the set-of-words model is too simple even for those.\n",
    "There are at least three problems:\n",
    "\n",
    "1. Context is not taken into account at all, even within individual sentences.\n",
    "   Among other things, *The dog bit the man* and *The man bit the dog* incorrectly receive the same meaning.\n",
    "   And along the same lines, *Not every student thinks they should leave* and *Every student thinks they should not leave* are taken to have identical meanings, too.\n",
    "1. Since we do not count how often words occur, a text that mentions global warming once in passing is taken to cover this topic to the same extent as one that mentions it over a hundred times.\n",
    "1. The sets are cluttered with uninformative words like *is*, *the*, *of*, and so on.\n",
    "\n",
    "The first one can be improved by moving from unigrams to $n$-grams.\n",
    "With bigrams, a headline like *man bites dogs* is represented as the set $\\setof{\\text{man bites}, \\text{bites dogs}}$, whereas the much less startling *dog bites man* becomes $\\setof{\\text{dog bites}, \\text{bites man}}$.\n",
    "Note that we could also include explicit markers \\$ to clearly identify the first and last word of a sentence.\n",
    "None of this is an adequate representation of context, but it nonetheless works fairly well in practice - though that might just be because the practical problems language technology is asked to solve nowadays are still fairly simple.\n",
    "\n",
    "Be that as it may, there are still problems 2 and 3 to take care of.\n",
    "But this is best left for the next few units."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
